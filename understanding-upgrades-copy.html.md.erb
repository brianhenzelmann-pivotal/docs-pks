---
title: About Enterprise PKS Upgrades
owner: PKS
---

This topic provides conceptual information about <%= vars.product_short %> upgrades,
including upgrading the PKS control plane and PKS-provisioned Kubernetes clusters.

For step-by-step instructions on upgrading <%= vars.product_short %>, see:

* [Upgrading Enterprise PKS](upgrade-pks.html)
* [Upgrading Enterprise PKS with NSX-T](upgrade-pks-nsxt.html)
* [Upgrading Clusters](upgrade-clusters.html)

## <a id="overview"></a>Overview

An <%= vars.product_short %> upgrade modifies the version of <%= vars.product_short %>,
for example, from v1.6.x to v1.7.0 or from v1.7.0 to v1.7.1.

By default, <%= vars.product_short %> is set to perform a full upgrade,
which upgrades both the PKS control plane and all PKS-provisioned Kubernetes clusters.

However, you can choose to upgrade <%= vars.product_short %> in two phases
by upgrading the PKS control plane first and
then upgrading your PKS-provisioned Kubernetes clusters later.

Both the full upgrade and the PKS control plane upgrade are performed
through the <%= vars.product_tile %> tile only.
When upgrading PKS-provisioned Kubernetes clusters, you can use either the <%= vars.product_tile %> tile or the PKS CLI.
See the table below.

<table>
<col width="50%">
<col width="25%">
<col width="25%">
  <tr>
    <th rowspan=2>Upgrade type</th>
    <th colspan=2 style="text-align:center">Upgrade method</th>    
  </tr>
  <tr>
    <th>PKS Tile</th>
    <th>PKS CLI</th>    
  </tr>
  <tr>
    <td>Full PKS upgrade</td>
    <td>&#10004;</td>
    <td>&#10006;</td>
  </tr>
  <tr>
    <td>PKS control plane only</td>
    <td>&#10004;</td>
    <td>&#10006;</td>
  </tr>
  <tr>
    <td>Kubernetes clusters only</td>
    <td>&#10004;</td>
    <td>&#10004;</td>
  </tr>
</table>

Typically, if you choose to upgrade PKS-provisioned Kubernetes clusters only,
you will upgrade them through the PKS CLI.

## <a id=""></a> Deciding Between Full and Two-Phase Upgrade

When deciding whether to perform the default full upgrade or
to upgrade the PKS control plane and PKS-provisioned Kubernetes separately,
consider your organization and <%= vars.product_short %> deployment needs.
For example, if your organization runs PKS-provisioned Kubernetes clusters
in both development and production environments and you want to upgrade
only one of the environments first, you can achieve your goal by
upgrading the PKS control plane and PKS-provisioned Kubernetes separately
instead of performing a full upgrade.

Examples of other advantages of
upgrading <%= vars.product_short %> in two phases include:

* Faster <%= vars.product_tile %> tile upgrades.
If you have a large number of clusters in your <%= vars.product_short %> deployment,
performing a full upgrade can significantly increase the amount of time required to
upgrade the <%= vars.product_tile %> tile.

* More granular control over cluster upgrades.
In addition to enabling you to upgrade subsets of clusters,
the PKS CLI supports upgrading each cluster individually.

* Not a monolithic upgrade.
This helps to isolate the root cause of an error when troubleshooting upgrades
and reduce the number of upgrade failures.
For example, when a cluster-related upgrade error occurs during a full upgrade,
the entire <%= vars.product_tile %> tile upgrade may fail.

<p class="note warning"><strong>Warning:</strong> If you disable the default full upgrade
and upgrade only the PKS control plane in the <%= vars.product_tile %>,
you must upgrade all your Kubernetes clusters before the next <%= vars.product_short %>
upgrade. If your Kubernetes clusters fall more than one version behind the PKS control plane and the tile,
<%= vars.product_short %> cannot upgrade the clusters.</p>

## <a id="what-happens"></a> What Happens During Full PKS and PKS Control Plane Upgrades

You can perform full PKS upgrades and PKS control plane upgrades only
through the <%= vars.product_tile %> tile.

After you add a new <%= vars.product_tile %> tile version to your staging area
on the Ops Manager Installation Dashboard,
Ops Manager automatically migrates your configuration settings into the new tile version.

For more information, see:

* [Full PKS Upgrades](#full-upgrades)
* [PKS Control Plane Upgrades](#control-plane-upgrades)

### <a id="full-upgrades"></a>Full PKS Upgrades

During a **full PKS upgrade**,
the <%= vars.product_tile %> tile does the following:

1. (Only in v1.7) Creates MySQL VM. When you first upgrade from <%= vars.product_short %> v1.6 to v1.7,
the upgrade process creates the <%= vars.control_plane_db %> VM, a new dedicated MySQL VM.
  - This MySQL VM resides alongside the <%= vars.control_plane %> VM on the PKS control plane.
  - The upgrade process then migrates the PKS v1.6 MySQL data from the <%= vars.control_plane %> VM to the new dedicated MySQL VM.
  - Subsequent <%= vars.product_short %> upgrades, from earlier to later patch versions of PKS v1.7, do not include this step.

1. Upgrades the <%= vars.control_plane %> VM, which hosts the PKS API and UAA servers.
This control plane upgrade causes temporary outages as described in [Control Plane Outages](#outages) below.

1. Upgrades PKS-provisioned Kubernetes clusters.
  * Upgrading <%= vars.product_short %> is controlled by the **Upgrade all clusters errand** in the <%= vars.product_tile %> tile.
  * The cluster upgrade process recreates all clusters, which may cause cluster outages.
  For more information, see the [What Happens During Cluster Upgrades](./upgrade-clusters.html#what) section of the _Upgrading Clusters_ topic.

### <a id="control-plane-upgrades"></a>PKS Control Plane Upgrades

When upgrading the **PKS control plane only**,
the <%= vars.product_tile %> tile follows the process described in
[Full PKS Upgrades](#full-upgrades) above, step 1 and 2.
It does not upgrade PKS-provisioned Kubernetes clusters, step 3.

### <a id="canary"></a>Canary Instances

The <%= vars.product_tile %> tile is a BOSH deployment.

BOSH-deployed products can set a number of canary instances to upgrade first, before the rest of the deployment VMs.
BOSH continues the upgrade only if the canary instance upgrade succeeds.
If the canary instance encounters an error, the upgrade stops running and other VMs are not affected.

The <%= vars.product_tile %> tile uses one canary instance when deploying or upgrading <%= vars.product_short %>.

#### <a id="outages"></a> Control Plane Outages

Upgrading the <%= vars.product_short %> control plane temporarily interrupts the following:

  * Logging in to the PKS CLI and using all `pks` commands
  * Using the PKS API to retrieve information about clusters
  * Using the PKS API to create and delete clusters
  * Using the PKS API to resize clusters

Recreating the PKS API server does not affect the Kubernetes clusters themselves.
During PKS control plane upgrade, you can still interact with clusters and their workloads using the Kubernetes Command Line Interface, `kubectl`.

For more information about the PKS control plane, see [PKS Control Plane Overview](control-plane.html#control-plane) in _<%= vars.product_short %> Architecture_.

## <a id="cluster-upgrades"></a>What Happens During Cluster Upgrades

Upgrading PKS-provisioned Kubernetes clusters
updates their Kubernetes version to the version
shipped with the <%= vars.product_tile %> tile.

During an upgrade of PKS-provisioned clusters,
performed either during a full PKS upgrade or separately through the PKS CLI,
<%= vars.product_short %> recreates your clusters, one at a time.
This includes the following stages for each cluster you upgrade:

1. Master nodes are upgraded.
1. Worker nodes are upgraded.

Depending on your cluster configuration,
these recreations may cause [Master Nodes Outage](#master) or [Worker Nodes Outage](#worker)
as described below.

###<a id="master"></a>Master Nodes Outage

When <%= vars.product_short %> upgrades a single-master cluster,
you cannot interact with your cluster, use `kubectl`, or push new workloads.

To avoid this loss of functionality,
<%= vars.recommended_by %> recommends using multi-master clusters.

###<a id="worker"></a>Worker Nodes Outage

When <%= vars.product_short %> upgrades a worker node,
the node stops running containers.
If your workloads run on a single node, they will experience downtime.

To avoid downtime for stateless workloads,
<%= vars.recommended_by %> recommends using at least one worker node per availability zone (AZ).
For stateful workloads, <%= vars.recommended_by %> recommends
using a minimum of two worker nodes per AZ.

<p class="note"><strong>Note</strong>: When <%= vars.product_short %> performs a full upgrade and upgrades <%= vars.product_short %>-provisioned clusters with a new Linux or Windows stemcell,
it rolls every Linux or Windows VM in each Kubernetes cluster.
This automatic rolling ensures that all your VMs are patched. To avoid workload downtime, use the resource configuration recommended in the <a href="#master-recreated">Master Nodes</a> and <a href="#worker-recreated">Worker Nodes</a> sections below and
in <a href="./maintain-uptime.html">Maintaining Workload Uptime</a>.</p>
